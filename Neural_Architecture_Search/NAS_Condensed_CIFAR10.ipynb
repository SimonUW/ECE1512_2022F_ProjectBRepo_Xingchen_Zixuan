{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6af9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15557c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "========================================\n",
      "Start training on the first LeNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 6.779597282409668 seconds\n",
      "Test accuracy is 24.32%\n",
      "Start training on the second LeNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 2.6851003170013428 seconds\n",
      "Test accuracy is 23.02%\n",
      "Start training on the third LeNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 2.6726951599121094 seconds\n",
      "Test accuracy is 22.939999999999998%\n",
      "----------------------------------------\n",
      "Average training time is 4.04579758644104 seconds\n",
      "Average test accuracy is 23.426666666666666%\n",
      "========================================\n",
      "Start training on the first AlexNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.721951007843018 seconds\n",
      "Test accuracy is 30.270000000000003%\n",
      "Start training on the second AlexNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.80364203453064 seconds\n",
      "Test accuracy is 29.39%\n",
      "Start training on the third AlexNet architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.576672792434692 seconds\n",
      "Test accuracy is 29.189999999999998%\n",
      "----------------------------------------\n",
      "Average training time is 4.70075527826945 seconds\n",
      "Average test accuracy is 29.616666666666664%\n",
      "========================================\n",
      "Start training on the first VGG11 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 6.272749662399292 seconds\n",
      "Test accuracy is 30.830000000000002%\n",
      "Start training on the second VGG11 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 6.239436626434326 seconds\n",
      "Test accuracy is 30.009999999999998%\n",
      "Start training on the third VGG11 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 6.003413438796997 seconds\n",
      "Test accuracy is 29.84%\n",
      "----------------------------------------\n",
      "Average training time is 6.171866575876872 seconds\n",
      "Average test accuracy is 30.22666666666667%\n",
      "========================================\n",
      "Start training on the first ConvNetD4 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.080846071243286 seconds\n",
      "Test accuracy is 38.3%\n",
      "Start training on the second ConvNetD4 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.055969953536987 seconds\n",
      "Test accuracy is 36.52%\n",
      "Start training on the third ConvNetD4 architecture on the condensed CIFAR10 dataset\n",
      "Training takes 4.212255477905273 seconds\n",
      "Test accuracy is 37.36%\n",
      "----------------------------------------\n",
      "Average training time is 4.116357167561849 seconds\n",
      "Average test accuracy is 37.39333333333333%\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, test_loader, optimizer, scheduler, criterion, epochs):\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "        for i_batch, datum in enumerate(train_loader):\n",
    "            img = datum[0].float().to(device)\n",
    "            lab = datum[1].long().to(device)\n",
    "            n_b = lab.shape[0]\n",
    "            output = model(img)\n",
    "            loss = criterion(output, lab)\n",
    "            acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "            loss_avg += loss.item()*n_b\n",
    "            acc_avg += acc\n",
    "            num_exp += n_b\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del img, lab\n",
    "                \n",
    "        loss_avg /= num_exp\n",
    "        acc_avg /= num_exp\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0   \n",
    "    model.eval()\n",
    "    for i_batch, datum in enumerate(test_loader):\n",
    "        img = datum[0].float().to(device)\n",
    "        lab = datum[1].long().to(device)\n",
    "        n_b = lab.shape[0]\n",
    "        output = model(img)\n",
    "        loss = criterion(output, lab)\n",
    "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += acc\n",
    "        num_exp += n_b\n",
    "        \n",
    "        del img, lab\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "    \n",
    "    return loss_avg, acc_avg\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform): # images: n x c x h x w tensor\n",
    "        self.images = images.detach().float()\n",
    "        self.labels = labels.detach()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.transform(self.images[index]), self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_path = './CIFAR10result/realres_DC_CIFAR10_ConvNet_10ipc.pt'\n",
    "data = torch.load(data_path)\n",
    "images = data['data'][0][0]\n",
    "\n",
    "CIFAR10_dataset = 'CIFAR10'\n",
    "CIFAR10_data_path = './CIFAR10data'\n",
    "CIFAR10_channel, CIFAR10_im_size, CIFAR10_num_classes, CIFAR10_class_names, CIFAR10_mean, CIFAR10_std, CIFAR10_dst_train, CIFAR10_dst_test, CIFAR10_testloader = get_dataset(CIFAR10_dataset, CIFAR10_data_path)\n",
    "condensed_labs_train = torch.ones(10*CIFAR10_num_classes)\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "for c in range(CIFAR10_num_classes):  \n",
    "    condensed_labs_train[c*10:(c+1)*10]*=c\n",
    "    \n",
    "condensed_train_dst = CustomDataset(images, condensed_labs_train, transform)\n",
    "CIFAR10_trainloader = torch.utils.data.DataLoader(condensed_train_dst, batch_size=8, shuffle=True, num_workers=0)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "model_set = ['LeNet', 'AlexNet', 'VGG11', 'ConvNetD4']\n",
    "iteration_set = ['first', 'second', 'third']\n",
    "for model_architecture in model_set:\n",
    "    print('========================================')\n",
    "    accs_each_model = []\n",
    "    training_times_each_model = []\n",
    "    for itr in iteration_set:\n",
    "        print('Start training on the '+itr+' '+model_architecture+' architecture on the condensed CIFAR10 dataset')\n",
    "        model = get_network(model_architecture, CIFAR10_channel, CIFAR10_num_classes, CIFAR10_im_size).to(device) # get a random model\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0)\n",
    "        start = time.time()\n",
    "        _, acc_avg = train(model, CIFAR10_trainloader , CIFAR10_testloader , optimizer, scheduler, criterion, epochs)\n",
    "        elapsed_time = time.time()-start\n",
    "        training_times_each_model.append(elapsed_time)\n",
    "        accs_each_model.append(acc_avg*100)\n",
    "        print('Training takes {} seconds'.format(training_times_each_model[-1]))\n",
    "        print('Test accuracy is {}%'.format(accs_each_model[-1]))\n",
    "    print('----------------------------------------')\n",
    "    print('Average training time is {} seconds'.format(sum(training_times_each_model)/len(training_times_each_model)))\n",
    "    print('Average test accuracy is {}%'.format(sum(accs_each_model)/len(accs_each_model)))\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
