{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aeccb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_it_pool:  [0, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "================== Exp 0 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DM', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 10, 'eval_mode': 'S', 'num_exp': 1, 'num_eval': 1, 'epoch_eval_train': 100, 'Iteration': 2000, 'lr_img': 1.0, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'real', 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': 'CIFAR10data', 'save_path': 'CIFAR10result', 'dis_metric': 'ours', 'outer_loop': 10, 'inner_loop': 50, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x000002457AD8FD00>, 'dsa': True}\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random real images\n",
      "[2022-12-01 20:10:59] training begins\n",
      "[2022-12-01 20:11:00] iter = 00000, loss = 25.6037\n",
      "[2022-12-01 20:11:02] iter = 00010, loss = 16.8999\n",
      "[2022-12-01 20:11:03] iter = 00020, loss = 12.6271\n",
      "[2022-12-01 20:11:05] iter = 00030, loss = 10.9261\n",
      "[2022-12-01 20:11:06] iter = 00040, loss = 9.4425\n",
      "[2022-12-01 20:11:08] iter = 00050, loss = 9.7971\n",
      "[2022-12-01 20:11:10] iter = 00060, loss = 8.7686\n",
      "[2022-12-01 20:11:11] iter = 00070, loss = 8.5124\n",
      "[2022-12-01 20:11:13] iter = 00080, loss = 8.1634\n",
      "[2022-12-01 20:11:14] iter = 00090, loss = 7.4516\n",
      "[2022-12-01 20:11:16] iter = 00100, loss = 7.5989\n",
      "[2022-12-01 20:11:17] iter = 00110, loss = 7.4659\n",
      "[2022-12-01 20:11:19] iter = 00120, loss = 7.7214\n",
      "[2022-12-01 20:11:20] iter = 00130, loss = 7.1622\n",
      "[2022-12-01 20:11:22] iter = 00140, loss = 7.1902\n",
      "[2022-12-01 20:11:23] iter = 00150, loss = 7.2052\n",
      "[2022-12-01 20:11:25] iter = 00160, loss = 6.6475\n",
      "[2022-12-01 20:11:26] iter = 00170, loss = 7.0405\n",
      "[2022-12-01 20:11:28] iter = 00180, loss = 6.8541\n",
      "[2022-12-01 20:11:29] iter = 00190, loss = 7.0500\n",
      "[2022-12-01 20:11:31] iter = 00200, loss = 6.7774\n",
      "[2022-12-01 20:11:33] iter = 00210, loss = 6.8631\n",
      "[2022-12-01 20:11:34] iter = 00220, loss = 6.4301\n",
      "[2022-12-01 20:11:36] iter = 00230, loss = 6.6989\n",
      "[2022-12-01 20:11:37] iter = 00240, loss = 6.6052\n",
      "[2022-12-01 20:11:39] iter = 00250, loss = 6.0784\n",
      "[2022-12-01 20:11:40] iter = 00260, loss = 6.2644\n",
      "[2022-12-01 20:11:42] iter = 00270, loss = 6.0388\n",
      "[2022-12-01 20:11:43] iter = 00280, loss = 5.8745\n",
      "[2022-12-01 20:11:45] iter = 00290, loss = 6.3901\n",
      "[2022-12-01 20:11:47] iter = 00300, loss = 5.7050\n",
      "[2022-12-01 20:11:48] iter = 00310, loss = 6.0025\n",
      "[2022-12-01 20:11:50] iter = 00320, loss = 6.0712\n",
      "[2022-12-01 20:11:51] iter = 00330, loss = 6.3788\n",
      "[2022-12-01 20:11:53] iter = 00340, loss = 5.9327\n",
      "[2022-12-01 20:11:54] iter = 00350, loss = 5.5817\n",
      "[2022-12-01 20:11:56] iter = 00360, loss = 5.8643\n",
      "[2022-12-01 20:11:57] iter = 00370, loss = 5.7660\n",
      "[2022-12-01 20:11:59] iter = 00380, loss = 5.4565\n",
      "[2022-12-01 20:12:01] iter = 00390, loss = 5.8930\n",
      "[2022-12-01 20:12:02] iter = 00400, loss = 6.4577\n",
      "[2022-12-01 20:12:04] iter = 00410, loss = 5.7006\n",
      "[2022-12-01 20:12:05] iter = 00420, loss = 5.3529\n",
      "[2022-12-01 20:12:07] iter = 00430, loss = 5.4018\n",
      "[2022-12-01 20:12:08] iter = 00440, loss = 5.3605\n",
      "[2022-12-01 20:12:10] iter = 00450, loss = 5.7868\n",
      "[2022-12-01 20:12:11] iter = 00460, loss = 5.9034\n",
      "[2022-12-01 20:12:13] iter = 00470, loss = 5.4423\n",
      "[2022-12-01 20:12:15] iter = 00480, loss = 5.2993\n",
      "[2022-12-01 20:12:16] iter = 00490, loss = 5.4909\n",
      "[2022-12-01 20:12:18] iter = 00500, loss = 5.6394\n",
      "[2022-12-01 20:12:19] iter = 00510, loss = 5.5205\n",
      "[2022-12-01 20:12:21] iter = 00520, loss = 5.3744\n",
      "[2022-12-01 20:12:23] iter = 00530, loss = 5.1779\n",
      "[2022-12-01 20:12:24] iter = 00540, loss = 5.4069\n",
      "[2022-12-01 20:12:26] iter = 00550, loss = 5.6116\n",
      "[2022-12-01 20:12:27] iter = 00560, loss = 5.3428\n",
      "[2022-12-01 20:12:29] iter = 00570, loss = 5.2104\n",
      "[2022-12-01 20:12:30] iter = 00580, loss = 5.5448\n",
      "[2022-12-01 20:12:32] iter = 00590, loss = 5.1325\n",
      "[2022-12-01 20:12:33] iter = 00600, loss = 5.4457\n",
      "[2022-12-01 20:12:35] iter = 00610, loss = 5.4188\n",
      "[2022-12-01 20:12:37] iter = 00620, loss = 5.1627\n",
      "[2022-12-01 20:12:38] iter = 00630, loss = 5.3204\n",
      "[2022-12-01 20:12:40] iter = 00640, loss = 5.0579\n",
      "[2022-12-01 20:12:41] iter = 00650, loss = 5.2879\n",
      "[2022-12-01 20:12:43] iter = 00660, loss = 5.2617\n",
      "[2022-12-01 20:12:45] iter = 00670, loss = 5.3197\n",
      "[2022-12-01 20:12:46] iter = 00680, loss = 5.1057\n",
      "[2022-12-01 20:12:48] iter = 00690, loss = 5.2098\n",
      "[2022-12-01 20:12:49] iter = 00700, loss = 5.1158\n",
      "[2022-12-01 20:12:51] iter = 00710, loss = 5.1649\n",
      "[2022-12-01 20:12:52] iter = 00720, loss = 5.0726\n",
      "[2022-12-01 20:12:54] iter = 00730, loss = 5.1675\n",
      "[2022-12-01 20:12:56] iter = 00740, loss = 5.2171\n",
      "[2022-12-01 20:12:57] iter = 00750, loss = 4.9540\n",
      "[2022-12-01 20:12:59] iter = 00760, loss = 4.9286\n",
      "[2022-12-01 20:13:00] iter = 00770, loss = 4.9568\n",
      "[2022-12-01 20:13:02] iter = 00780, loss = 4.8402\n",
      "[2022-12-01 20:13:04] iter = 00790, loss = 5.0129\n",
      "[2022-12-01 20:13:05] iter = 00800, loss = 5.0893\n",
      "[2022-12-01 20:13:07] iter = 00810, loss = 4.6942\n",
      "[2022-12-01 20:13:08] iter = 00820, loss = 5.1510\n",
      "[2022-12-01 20:13:10] iter = 00830, loss = 5.1139\n",
      "[2022-12-01 20:13:11] iter = 00840, loss = 4.9263\n",
      "[2022-12-01 20:13:13] iter = 00850, loss = 5.0258\n",
      "[2022-12-01 20:13:15] iter = 00860, loss = 5.2108\n",
      "[2022-12-01 20:13:16] iter = 00870, loss = 4.9118\n",
      "[2022-12-01 20:13:18] iter = 00880, loss = 4.9261\n",
      "[2022-12-01 20:13:19] iter = 00890, loss = 4.9425\n",
      "[2022-12-01 20:13:21] iter = 00900, loss = 5.0778\n",
      "[2022-12-01 20:13:22] iter = 00910, loss = 4.8211\n",
      "[2022-12-01 20:13:24] iter = 00920, loss = 5.1724\n",
      "[2022-12-01 20:13:26] iter = 00930, loss = 4.7198\n",
      "[2022-12-01 20:13:27] iter = 00940, loss = 5.0144\n",
      "[2022-12-01 20:13:29] iter = 00950, loss = 5.1211\n",
      "[2022-12-01 20:13:30] iter = 00960, loss = 4.8654\n",
      "[2022-12-01 20:13:32] iter = 00970, loss = 4.7439\n",
      "[2022-12-01 20:13:34] iter = 00980, loss = 5.0299\n",
      "[2022-12-01 20:13:35] iter = 00990, loss = 4.9095\n",
      "[2022-12-01 20:13:37] iter = 01000, loss = 4.8754\n",
      "[2022-12-01 20:13:38] iter = 01010, loss = 4.9485\n",
      "[2022-12-01 20:13:40] iter = 01020, loss = 4.4280\n",
      "[2022-12-01 20:13:41] iter = 01030, loss = 5.0904\n",
      "[2022-12-01 20:13:43] iter = 01040, loss = 4.7865\n",
      "[2022-12-01 20:13:45] iter = 01050, loss = 4.9764\n",
      "[2022-12-01 20:13:46] iter = 01060, loss = 4.9372\n",
      "[2022-12-01 20:13:48] iter = 01070, loss = 4.5552\n",
      "[2022-12-01 20:13:49] iter = 01080, loss = 5.0437\n",
      "[2022-12-01 20:13:51] iter = 01090, loss = 4.8889\n",
      "[2022-12-01 20:13:52] iter = 01100, loss = 4.8162\n",
      "[2022-12-01 20:13:54] iter = 01110, loss = 4.8543\n",
      "[2022-12-01 20:13:56] iter = 01120, loss = 4.7687\n",
      "[2022-12-01 20:13:57] iter = 01130, loss = 4.8754\n",
      "[2022-12-01 20:13:59] iter = 01140, loss = 4.6540\n",
      "[2022-12-01 20:14:00] iter = 01150, loss = 4.7772\n",
      "[2022-12-01 20:14:02] iter = 01160, loss = 4.6058\n",
      "[2022-12-01 20:14:03] iter = 01170, loss = 4.7395\n",
      "[2022-12-01 20:14:05] iter = 01180, loss = 4.7803\n",
      "[2022-12-01 20:14:07] iter = 01190, loss = 4.8565\n",
      "[2022-12-01 20:14:08] iter = 01200, loss = 4.7516\n",
      "[2022-12-01 20:14:10] iter = 01210, loss = 4.7287\n",
      "[2022-12-01 20:14:11] iter = 01220, loss = 4.9144\n",
      "[2022-12-01 20:14:13] iter = 01230, loss = 4.7722\n",
      "[2022-12-01 20:14:15] iter = 01240, loss = 4.6668\n",
      "[2022-12-01 20:14:16] iter = 01250, loss = 4.7834\n",
      "[2022-12-01 20:14:18] iter = 01260, loss = 4.8290\n",
      "[2022-12-01 20:14:19] iter = 01270, loss = 4.9447\n",
      "[2022-12-01 20:14:21] iter = 01280, loss = 4.5407\n",
      "[2022-12-01 20:14:22] iter = 01290, loss = 4.4737\n",
      "[2022-12-01 20:14:24] iter = 01300, loss = 4.8664\n",
      "[2022-12-01 20:14:26] iter = 01310, loss = 4.5787\n",
      "[2022-12-01 20:14:27] iter = 01320, loss = 4.5771\n",
      "[2022-12-01 20:14:29] iter = 01330, loss = 4.8461\n",
      "[2022-12-01 20:14:30] iter = 01340, loss = 4.4953\n",
      "[2022-12-01 20:14:32] iter = 01350, loss = 4.5128\n",
      "[2022-12-01 20:14:34] iter = 01360, loss = 4.6768\n",
      "[2022-12-01 20:14:35] iter = 01370, loss = 4.6018\n",
      "[2022-12-01 20:14:37] iter = 01380, loss = 4.4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-12-01 20:14:38] iter = 01390, loss = 4.8749\n",
      "[2022-12-01 20:14:40] iter = 01400, loss = 4.8846\n",
      "[2022-12-01 20:14:42] iter = 01410, loss = 4.6082\n",
      "[2022-12-01 20:14:43] iter = 01420, loss = 4.5289\n",
      "[2022-12-01 20:14:45] iter = 01430, loss = 4.7989\n",
      "[2022-12-01 20:14:46] iter = 01440, loss = 4.5239\n",
      "[2022-12-01 20:14:48] iter = 01450, loss = 4.4418\n",
      "[2022-12-01 20:14:50] iter = 01460, loss = 4.6566\n",
      "[2022-12-01 20:14:51] iter = 01470, loss = 5.0622\n",
      "[2022-12-01 20:14:53] iter = 01480, loss = 4.4815\n",
      "[2022-12-01 20:14:54] iter = 01490, loss = 4.1829\n",
      "[2022-12-01 20:14:56] iter = 01500, loss = 4.4291\n",
      "[2022-12-01 20:14:58] iter = 01510, loss = 4.7789\n",
      "[2022-12-01 20:14:59] iter = 01520, loss = 4.6198\n",
      "[2022-12-01 20:15:01] iter = 01530, loss = 4.6070\n",
      "[2022-12-01 20:15:02] iter = 01540, loss = 4.7921\n",
      "[2022-12-01 20:15:04] iter = 01550, loss = 4.4847\n",
      "[2022-12-01 20:15:05] iter = 01560, loss = 4.5634\n",
      "[2022-12-01 20:15:07] iter = 01570, loss = 4.4424\n",
      "[2022-12-01 20:15:09] iter = 01580, loss = 4.8564\n",
      "[2022-12-01 20:15:10] iter = 01590, loss = 4.5361\n",
      "[2022-12-01 20:15:12] iter = 01600, loss = 4.6877\n",
      "[2022-12-01 20:15:13] iter = 01610, loss = 4.5372\n",
      "[2022-12-01 20:15:15] iter = 01620, loss = 4.7535\n",
      "[2022-12-01 20:15:17] iter = 01630, loss = 4.6802\n",
      "[2022-12-01 20:15:18] iter = 01640, loss = 4.4809\n",
      "[2022-12-01 20:15:20] iter = 01650, loss = 4.7830\n",
      "[2022-12-01 20:15:21] iter = 01660, loss = 4.4053\n",
      "[2022-12-01 20:15:23] iter = 01670, loss = 4.8312\n",
      "[2022-12-01 20:15:25] iter = 01680, loss = 4.7749\n",
      "[2022-12-01 20:15:26] iter = 01690, loss = 4.6311\n",
      "[2022-12-01 20:15:28] iter = 01700, loss = 4.4075\n",
      "[2022-12-01 20:15:29] iter = 01710, loss = 4.3451\n",
      "[2022-12-01 20:15:31] iter = 01720, loss = 4.9160\n",
      "[2022-12-01 20:15:32] iter = 01730, loss = 4.5868\n",
      "[2022-12-01 20:15:34] iter = 01740, loss = 4.5639\n",
      "[2022-12-01 20:15:36] iter = 01750, loss = 4.5666\n",
      "[2022-12-01 20:15:37] iter = 01760, loss = 4.4297\n",
      "[2022-12-01 20:15:39] iter = 01770, loss = 4.5840\n",
      "[2022-12-01 20:15:40] iter = 01780, loss = 4.3871\n",
      "[2022-12-01 20:15:42] iter = 01790, loss = 4.5499\n",
      "[2022-12-01 20:15:43] iter = 01800, loss = 4.7542\n",
      "[2022-12-01 20:15:45] iter = 01810, loss = 4.6489\n",
      "[2022-12-01 20:15:47] iter = 01820, loss = 4.4206\n",
      "[2022-12-01 20:15:48] iter = 01830, loss = 4.4985\n",
      "[2022-12-01 20:15:50] iter = 01840, loss = 4.6778\n",
      "[2022-12-01 20:15:51] iter = 01850, loss = 4.3604\n",
      "[2022-12-01 20:15:53] iter = 01860, loss = 4.3979\n",
      "[2022-12-01 20:15:55] iter = 01870, loss = 4.4003\n",
      "[2022-12-01 20:15:56] iter = 01880, loss = 4.8586\n",
      "[2022-12-01 20:15:58] iter = 01890, loss = 4.6956\n",
      "[2022-12-01 20:15:59] iter = 01900, loss = 4.5239\n",
      "[2022-12-01 20:16:01] iter = 01910, loss = 4.4454\n",
      "[2022-12-01 20:16:03] iter = 01920, loss = 4.6897\n",
      "[2022-12-01 20:16:04] iter = 01930, loss = 4.3251\n",
      "[2022-12-01 20:16:06] iter = 01940, loss = 4.4335\n",
      "[2022-12-01 20:16:07] iter = 01950, loss = 4.1716\n",
      "[2022-12-01 20:16:09] iter = 01960, loss = 4.5801\n",
      "[2022-12-01 20:16:10] iter = 01970, loss = 4.6112\n",
      "[2022-12-01 20:16:12] iter = 01980, loss = 4.7462\n",
      "[2022-12-01 20:16:14] iter = 01990, loss = 4.1859\n",
      "[2022-12-01 20:16:15] iter = 02000, loss = 4.5205\n",
      "\n",
      "==================== Final Results ====================\n",
      "\n",
      "After 2000 iterations, the model test accuracy on synthetic data is 45.16%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import tqdm\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "from utils import get_loops, get_dataset, get_network, get_eval_pool, evaluate_synset, get_daparam, match_loss, get_time, TensorDataset, epoch, DiffAugment, ParamDiffAug\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "def main(args):\n",
    "    if not os.path.exists(args.data_path):\n",
    "        os.mkdir(args.data_path)\n",
    "\n",
    "    if not os.path.exists(args.save_path):\n",
    "        os.mkdir(args.save_path)\n",
    "\n",
    "    eval_it_pool = np.arange(0, args.Iteration+1, 200).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration] # The list of iterations when we evaluate models and record results.\n",
    "    print('eval_it_pool: ', eval_it_pool)\n",
    "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "\n",
    "    data_save = []\n",
    "\n",
    "    for exp in range(args.num_exp):\n",
    "        print('\\n================== Exp %d ==================\\n '%exp)\n",
    "        print('Hyper-parameters: \\n', args.__dict__)\n",
    "\n",
    "        ''' organize the real dataset '''\n",
    "        images_all = []\n",
    "        labels_all = []\n",
    "        indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "        images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "        labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "        for i, lab in enumerate(labels_all):\n",
    "            indices_class[lab].append(i)\n",
    "        images_all = torch.cat(images_all, dim=0).to(args.device)\n",
    "        labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
    "\n",
    "\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
    "\n",
    "        def get_images(c, n): # get random n images from class c\n",
    "            idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "            return images_all[idx_shuffle]\n",
    "\n",
    "        for ch in range(channel):\n",
    "            print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
    "\n",
    "\n",
    "        ''' initialize the synthetic data '''\n",
    "        image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n",
    "        label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
    "\n",
    "        if args.init == 'real':\n",
    "            print('initialize synthetic data from random real images')\n",
    "            for c in range(num_classes):\n",
    "                image_syn.data[c*args.ipc:(c+1)*args.ipc] = get_images(c, args.ipc).detach().data\n",
    "        else:\n",
    "            print('initialize synthetic data from random noise')\n",
    "\n",
    "\n",
    "        ''' training '''\n",
    "        optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5) # optimizer_img for synthetic data\n",
    "        optimizer_img.zero_grad()\n",
    "        print('%s training begins'%get_time())\n",
    "\n",
    "        for it in range(args.Iteration+1):\n",
    "            ''' Train synthetic data '''\n",
    "            net = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "            net.train()\n",
    "            for param in list(net.parameters()):\n",
    "                param.requires_grad = False\n",
    "\n",
    "            embed = net.module.embed if torch.cuda.device_count() > 1 else net.embed # for GPU parallel\n",
    "\n",
    "            loss_avg = 0\n",
    "\n",
    "            ''' update synthetic data '''\n",
    "            if 'BN' not in args.model: # for ConvNet\n",
    "                loss = torch.tensor(0.0).to(args.device)\n",
    "                for c in range(num_classes):\n",
    "                    img_real = get_images(c, args.batch_real)\n",
    "                    img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                    if args.dsa:\n",
    "                        seed = int(time.time() * 1000) % 100000\n",
    "                        img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                    output_real = embed(img_real).detach()\n",
    "                    output_syn = embed(img_syn)\n",
    "#                     import pdb;pdb.set_trace()\n",
    "                    loss += torch.sum((torch.mean(output_real, dim=0) - torch.mean(output_syn, dim=0))**2)\n",
    "\n",
    "            else: # for ConvNetBN\n",
    "                images_real_all = []\n",
    "                images_syn_all = []\n",
    "                loss = torch.tensor(0.0).to(args.device)\n",
    "                for c in range(num_classes):\n",
    "                    img_real = get_images(c, args.batch_real)\n",
    "                    img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                    if args.dsa:\n",
    "                        seed = int(time.time() * 1000) % 100000\n",
    "                        img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                    images_real_all.append(img_real)\n",
    "                    images_syn_all.append(img_syn)\n",
    "\n",
    "                images_real_all = torch.cat(images_real_all, dim=0)\n",
    "                images_syn_all = torch.cat(images_syn_all, dim=0)\n",
    "\n",
    "                output_real = embed(images_real_all).detach()\n",
    "                output_syn = embed(images_syn_all)\n",
    "\n",
    "                loss += torch.sum((torch.mean(output_real.reshape(num_classes, args.batch_real, -1), dim=1) - torch.mean(output_syn.reshape(num_classes, args.ipc, -1), dim=1))**2)\n",
    "\n",
    "\n",
    "\n",
    "            optimizer_img.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_img.step()\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "\n",
    "            loss_avg /= (num_classes)\n",
    "\n",
    "            if it%10 == 0:\n",
    "                print('%s iter = %05d, loss = %.4f' % (get_time(), it, loss_avg))\n",
    "\n",
    "            if it == args.Iteration: # only record the final results\n",
    "                data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "                torch.save({'data': data_save}, os.path.join(args.save_path, 'res_%s_%s_%s_%dipc.pt'%(args.method, args.dataset, args.model, args.ipc)))\n",
    "                        \n",
    "            ''' Evaluate synthetic data '''\n",
    "            if it == eval_it_pool[-1]:\n",
    "                net_eval = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
    "                image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach()) # avoid any unaware modification\n",
    "                _, acc_train, acc_test = evaluate_synset(net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "                print('\\n==================== Final Results ====================\\n')\n",
    "                print('After {} iterations, the model test accuracy on synthetic data is {}%'.format(it, acc_test*100))\n",
    "            \n",
    "            if it in eval_it_pool:    \n",
    "                ''' visualize and save '''\n",
    "                save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_exp%d_iter%d.png'%(args.method, args.dataset, args.model, args.ipc, exp, it))\n",
    "                image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "                for ch in range(channel):\n",
    "                    image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
    "                image_syn_vis[image_syn_vis<0] = 0.0\n",
    "                image_syn_vis[image_syn_vis>1] = 1.0\n",
    "                save_image(image_syn_vis, save_name, nrow=args.ipc) # Trying normalize = True/False may get better visual effects.\n",
    "\n",
    "class arguments():\n",
    "    def __init__(self,): \n",
    "        self.method = 'DM'\n",
    "        self.dataset = 'CIFAR10'\n",
    "        self.model = 'ConvNet'\n",
    "        #'image(s) per class'\n",
    "        self.ipc = 10\n",
    "         # S: the same to training model, M: multi architectures,  W: net width, D: net depth, A: activation function, P: pooling layer, N: normalization layer,\n",
    "        self.eval_mode = 'S'\n",
    "        #the number of experiments\n",
    "        self.num_exp = 1\n",
    "        #the number of evaluating randomly initialized models\n",
    "        self.num_eval = 1\n",
    "        #c\n",
    "        self.epoch_eval_train = 100\n",
    "        #training iterations\n",
    "        self.Iteration = 2000\n",
    "        self.lr_img = 1.0\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_real = 256\n",
    "        self.batch_train = 256\n",
    "        #'noise/real: initialize synthetic images from random noise or randomly sampled real images.'\n",
    "        self.init = 'real'\n",
    "        self.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n",
    "        self.data_path = 'CIFAR10data'\n",
    "        self.save_path = 'CIFAR10result'\n",
    "        self.dis_metric = 'ours'\n",
    "        self.outer_loop = 10\n",
    "        self.inner_loop = 50\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.dsa_param = ParamDiffAug()\n",
    "        self.dsa = True\n",
    "        \n",
    "args = arguments()\n",
    "main(args)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f48f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
